{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WKwG-pShM7mx"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pydicom\n","  Downloading pydicom-2.3.0-py3-none-any.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 4.8 MB/s \n","\u001b[?25hInstalling collected packages: pydicom\n","Successfully installed pydicom-2.3.0\n"]}],"source":["!pip install pydicom"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_xdonOfUNC7t"},"outputs":[],"source":["from glob import glob\n","from sklearn.model_selection import GroupKFold, StratifiedKFold\n","import cv2\n","from skimage import io\n","import os\n","from datetime import datetime\n","import time\n","import random\n","import cv2\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from IPython.display import display\n","import matplotlib.pyplot as plt\n","import sklearn\n","import warnings\n","import joblib\n","from sklearn.metrics import roc_auc_score, log_loss\n","from sklearn import metrics\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","import warnings; warnings.filterwarnings('ignore')\n","import cv2\n","import pydicom\n","#from efficientnet_pytorch import EfficientNet\n","from scipy.ndimage.interpolation import zoom\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score\n","\n","from pylab import rcParams\n","rcParams['figure.figsize'] = 20,5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCka5X4LNC4O"},"outputs":[],"source":["!gdown --id 1diztKyKFfhINpY-8R_KB84kV8akW2NjD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CItEZO3JNC0g"},"outputs":[],"source":["!unzip -qq test_train_val.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ung9-N4mNCwQ"},"outputs":[],"source":["with open('test_train_val/train_meta.csv') as f:\n","  train_df= pd.read_csv(f)\n","with open('test_train_val/train_signal.csv') as f:\n","  train_signal = pd.read_csv(f)\n","\n","with open('test_train_val/valid_meta.csv') as f:\n","  valid_df= pd.read_csv(f)\n","with open('test_train_val/valid_signal.csv') as f:\n","  valid_signal = pd.read_csv(f)\n","\n","with open('test_train_val/test_meta.csv') as f:\n","  test_df= pd.read_csv(f)\n","with open('test_train_val/test_signal.csv') as f:\n","  test_signal = pd.read_csv(f)\n","\n","print(train_df.shape)\n","\n","train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK-MWkcjNCtR"},"outputs":[],"source":["class PTBXLDatasetPreprocesser():\n","    def __init__(self):\n","        pass\n","    \n","    def save(self, filename):\n","        data = {\n","            'superclass_cols': self.superclass_cols,\n","            'subclass_cols': self.subclass_cols,\n","            'meta_num_cols': self.meta_num_cols,\n","            'meta_num_means': self.meta_num_means,\n","            'min_max_scaler': self.min_max_scaler,\n","            'meta_cat_cols': self.meta_cat_cols,\n","            'cat_lablers': self.cat_lablers,\n","            'bclass_cols': self.bclass_cols,\n","            \n","        }\n","        pd.to_pickle(data, filename)\n","        \n","    def load(self, filename):\n","        data = pd.read_pickle(filename)\n","        self.min_max_scaler = data['min_max_scaler']\n","        self.cat_lablers = data['cat_lablers']\n","        #self.binary_lablers = data['binary_lablers']\n","        \n","    def fit(self, x, y):\n","        x = x.copy()\n","        y = y.copy()\n","        \n","        self.superclass_cols = [ 'MI', 'STTC', 'CD', 'HYP']\n","        \n","        self.subclass_cols = [col for col in y.columns if 'sub_' in col]\n","\n","        self.bclass_cols = ['NORM']\n","        \n","        self.meta_num_cols = ['age', 'height', 'weight']\n","        self.meta_num_means = []\n","        for col in self.meta_num_cols:\n","            print(col, y[col].mean())\n","            y[col] = y[col].fillna(y[col].mean())\n","            self.meta_num_means += [y[col].mean()]\n","            \n","        self.min_max_scaler = MinMaxScaler().fit(y[self.meta_num_cols])\n","        \n","        self.meta_cat_cols = ['sex'] #, 'nurse', 'device']\n","        self.cat_lablers = [LabelEncoder().fit(y[col].fillna('none').astype(str)) for col in self.meta_cat_cols]\n","        return self\n","\n","        #self.min_max_scaler = MinMaxScaler().fit(y[self.meta_num_cols])\n","        \n","        #self.meta_binary_cols = ['NORM'] \n","        #self.binary_lablers = [LabelEncoder().fit(y[col].fillna('none').astype(str)) for col in self.meta_binary_cols]\n","        #return self\n","    \n","    def transform(self, x, y):\n","        \n","        channel_cols = x.columns.tolist()[1:]\n","        \n","        ret = []\n","        x = x[channel_cols].values.reshape(-1, 1000, 12)\n","        print(x.shape)\n","        ret += [x] # signal\n","        \n","        y_ = y.copy()\n","        \n","        for i, col in enumerate(self.meta_num_cols):\n","            y_[col] = y_[col].fillna(self.meta_num_means[i])\n","        y_[self.meta_num_cols] = self.min_max_scaler.transform(y_[self.meta_num_cols])\n","        y_[self.meta_num_cols] = np.clip(y_[self.meta_num_cols], 0., 1.) # prevent extreme value far from train set\n","        \n","        ret += [y_[self.meta_num_cols]] # meta num features\n","        \n","        for i, col in enumerate(self.meta_cat_cols):\n","            y_[col] = y_[col].fillna('none').astype(str)\n","            y_[col] = self.cat_lablers[i].transform(y_[col]) \n","        \n","        ret += [y_[self.meta_cat_cols]] # meta cat features\n","\n","        #for i, col in enumerate(self.meta_binary_cols):\n","           # y_[col] = y_[col].fillna('none').astype(str)\n","           # y_[col] = self.binary_lablers[i].transform(y_[col]) \n","        \n","        #ret += [y_[self.meta_binary_cols]] # binary class target\n","        \n","        if np.isin(self.superclass_cols, y.columns).sum() == len(self.superclass_cols):\n","            ret += [y[self.superclass_cols].fillna(0).astype(int)] # superclass targets\n","        \n","        if np.isin(self.subclass_cols, y.columns).sum() == len(self.subclass_cols):\n","            ret += [y[self.subclass_cols].fillna(0).astype(int)] # subclass targets\n","\n","        if np.isin(self.bclass_cols, y.columns).sum() == len(self.bclass_cols):\n","            ret += [y[self.bclass_cols].fillna(0).astype(int)]\n","        \n","        return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5ztEZWRNCqj"},"outputs":[],"source":["data_preprocessor = PTBXLDatasetPreprocesser()\n","data_preprocessor.fit(train_signal, train_df)\n","train_signal, train_meta_num_feats, train_meta_cat_feats, train_superclass, train_subclass, train_bclass = data_preprocessor.transform(train_signal, train_df)\n","valid_signal, valid_meta_num_feats, valid_meta_cat_feats, valid_superclass, valid_subclass, valid_bclass = data_preprocessor.transform(valid_signal, valid_df)\n","test_signal, test_meta_num_feats, test_meta_cat_feats, test_superclass, test_subclass, test_bclass = data_preprocessor.transform(test_signal, test_df)\n","\n","print(train_signal.shape)\n","print(valid_meta_num_feats.isna().sum(), valid_meta_cat_feats.isna().sum(), valid_superclass.isna().sum(), valid_subclass.isna().sum(), valid_bclass.isna().sum())\n","\n","display(train_meta_num_feats)\n","display(train_meta_cat_feats)\n","display(train_superclass)\n","display(train_subclass)\n","display(train_bclass)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WK2fKCogNCnY"},"outputs":[],"source":["signal=np.array(train_signal)\n","signal.shape\n","#print(signal)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCaZakB5NXrj"},"outputs":[],"source":["target=np.array(train_bclass)\n","target.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4vNO_JfNXoV"},"outputs":[],"source":["v_signal=np.array(valid_signal)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CV79-QtyNXlU"},"outputs":[],"source":["v_signal.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DuOCwguNXil"},"outputs":[],"source":["v_target=np.array(valid_bclass)\n","v_target.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28PMHuG_NXf-"},"outputs":[],"source":["!pip install ecg-plot\n","#import physionet_challenge_utility_script as pc\n","import ecg_plot\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","#from keras.utils import plot_model\n","#from keras.preprocessing.sequence import pad_sequences\n","from keras import layers\n","from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\n","from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPool1D, ZeroPadding1D, LSTM, Bidirectional, ConvLSTM1D\n","from keras.models import Sequential, Model\n","#from keras.utils import plot_model\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from keras.layers import concatenate\n","from scipy import optimize\n","from scipy.io import loadmat\n","import os\n","%load_ext autoreload\n","%autoreload\n","%reload_ext autoreload"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC636BiwNXdA"},"outputs":[],"source":["reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","    monitor='val_accuracy', factor=0.1, patience=10\n",")\n","\n","early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=31)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoTAgnOPNnL-"},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D\n","from keras.layers.normalization import BatchNormalization\n","from keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n","from keras.optimizers import Adam # I believe this is better optimizer for our case\n","from keras.preprocessing.image import ImageDataGenerator # to augmenting our images for increasing accuracy\n","from keras.utils.vis_utils import plot_model\n","import scipy\n","from sklearn.model_selection import train_test_split # to split our train data into train and validation sets\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","np.random.seed(13) # My lucky number"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oG78pfQCNnIm"},"outputs":[],"source":["num_classes = 10 # We have 10 digits to identify\n","batch_size = 128 # Handle 128 pictures at each round\n","epochs = 700 \n","img_rows, img_cols = 28, 28 # Image dimensions 28 pixels in height\u0026width\n","input_shape = (img_rows, img_cols,1) # We'll use this while building layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayLOv8J7NnF2"},"outputs":[],"source":["def model_cnn(input_shape=input_shape, num_classes=num_classes):   \n","    model = Sequential()\n","\n","    # Add convolutional layer consisting of 32 filters and shape of 3x3 with ReLU activation\n","    # We want to preserve more information for following layers so we use padding\n","    # 'Same' padding tries to pad evenly left and right, \n","    # but if the amount of columns to be added is odd, it will add the extra column to the right\n","    model.add(Conv2D(32, kernel_size = (3,3), activation='relu', input_shape = input_shape))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(32, kernel_size = (3,3), activation='relu'))\n","    model.add(BatchNormalization())\n","\n","    # Add convolutional layer consisting of 32 filters and shape of 5x5 with ReLU activation\n","    # We give strides=2 for space between each sample on the pixel grid\n","    model.add(Conv2D(32, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n","    model.add(BatchNormalization())\n","    # Dropping %40 of neurons\n","    model.add(Dropout(0.4))\n","    \n","    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(64, kernel_size = (3,3), activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(64, kernel_size = (5,5), strides=2, padding='same', activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.4))\n","\n","    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n","    model.add(BatchNormalization())\n","    # To be able to merge into fully connected layer we have to flatten\n","    model.add(Flatten())\n","    model.add(Dropout(0.4))\n","    # Lets add softmax activated neurons as much as number of classes\n","    model.add(Dense(num_classes, activation = \"softmax\"))\n","    # Compile the model with loss and metrics\n","    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMQKrEuZNnC_"},"outputs":[],"source":["def LeNet5(input_shape=input_shape,num_classes=num_classes):\n","    model = Sequential()\n","    model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape, padding=\"same\"))\n","    model.add(AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n","    model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n","    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n","    model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='valid'))\n","    model.add(Flatten())\n","    model.add(Dense(84, activation='relu'))\n","    model.add(Dense(num_classes, activation='softmax'))\n","    model.compile(optimizer =  Adam() , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UOhY-SCefoq"},"outputs":[],"source":["print(\"My Custom CNN Network:\")\n","plot_model(model_cnn(), to_file='custom-cnn.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PITpDc57eflN"},"outputs":[],"source":["print(\"Master Yann LeCun's LeNet-5 Network:\")\n","plot_model(LeNet5(), to_file='lenet-5.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUfXDqAPefit"},"outputs":[],"source":["model = []\n","model.append(model_cnn())\n","model.append(LeNet5())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNvsKdaCefgG"},"outputs":[],"source":["# Generate batches of tensor image data with real-time data augmentation more detail: https://keras.io/preprocessing/image/\n","datagen = ImageDataGenerator(rotation_range=10, zoom_range = 0.1, width_shift_range=0.1, height_shift_range=0.1)\n","datagen.fit(x_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lX58Zf8eefdP"},"outputs":[],"source":["# Start multiple model training with the batch size\n","models = []\n","for i in range(len(model)):\n","    model[i].fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n","                                        epochs = epochs, steps_per_epoch=x_train.shape[0] // batch_size,\n","                                        validation_data = (x_test,y_test), \n","                                        callbacks=[ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1)], \n","                                        verbose=2)\n","    models.append(model[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdGUjjTAe3CQ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBgXyayGe2-3"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3JwVqhBe28p"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6MaGRpLe26I"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VND5sjAce23Z"},"outputs":[],"source":[""]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOghK3FnQDFv9+UIT7wzI0u","name":"ensemble.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}